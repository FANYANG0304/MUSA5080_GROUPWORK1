---
title: "phase3"
format: html
editor: visual
---

## Quarto

Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see <https://quarto.org>.

## Running Code

When you click the **Render** button a document will be generated that includes both content and the output of embedded code. You can embed code like this:

```{r}


#| message: false
#| warning: false

# Step 1 — Load packages and data, basic checks

library(tidyverse)
library(lubridate)

# load dataset (adjust path if needed)

df <- readr::read_csv("data2/opa_sales_final_complete.csv", show_col_types = FALSE)

# basic info

dim(df)
glimpse(df)
colSums(is.na(df)) |> sort(decreasing = TRUE) |> head(10)



```



```{r}
# Step 2 — Target variable inspection (sale_price vs. log)

df <- df |>
mutate(
log_sale_price = log(sale_price)
)

# distribution plots

p1 <- ggplot(df, aes(sale_price)) +
geom_histogram(bins = 50, fill = "steelblue", color = "white") +
labs(title = "Distribution of Sale Price", x = "Sale Price ($)", y = "Count")

p2 <- ggplot(df, aes(log_sale_price)) +
geom_histogram(bins = 50, fill = "seagreen", color = "white") +
labs(title = "Distribution of log(Sale Price)", x = "log(Sale Price)", y = "Count")

p1
p2

# simple skewness check without external package

skewness <- function(x) {
x <- x[!is.na(x)]
n <- length(x)
m3 <- mean((x - mean(x))^3)
m2 <- mean((x - mean(x))^2)
(n * m3) / ((n - 1) * (n - 2) * (m2^(3/2)))
}

skew_raw <- skewness(df$sale_price)
skew_log <- skewness(df$log_sale_price)
tibble(skew_raw, skew_log)

```
```{r}


```

```{r}


# Step 3 — Feature engineering (updated, no log transform for crime)
df_fe <- df |>
  mutate(
    # house age
    house_age = 2025 - year_built,
    # time features
    sale_date = suppressWarnings(lubridate::ymd(sale_date)),
    sale_year = year(sale_date),
    sale_quarter = quarter(sale_date),
    # log1p transforms for only highly skewed distance variables
    dist_to_park_ft_log          = log1p(dist_to_park_ft),
    dist_transit_ft_log          = log1p(dist_transit_ft),
    dist_to_hospital_ft_log      = log1p(dist_to_hospital_ft),
    dist_to_nearest_school_ft_log= log1p(dist_to_nearest_school_ft)
  )

glimpse(df_fe)


```

```{r}
# Step 4 — Assemble modeling dataset and scale continuous variables (updated)

# continuous variables to scale (use _log names from Step 3; keep crime_count raw)
cont_vars <- c(
  "number_of_bedrooms","number_of_bathrooms","total_livable_area","house_age",
  "median_incomeE","per_cap_incomeE","PCTPOVERTY","PCBACHMORE",
  "dist_to_park_ft_log","dist_transit_ft_log","dist_to_hospital_ft_log",
  "dist_to_nearest_school_ft_log","crime_count_15min_walk"
)

# other predictors (not scaled)
other_vars <- c(
  "exterior_condition","interior_condition",
  "park_within_15min_walk","transit_15min_walk",
  "hospitals_15min_walk","schools_within_15min_walk",
  "sale_year","sale_quarter"
)

target <- "log_sale_price"

# select only existing columns (avoids errors if any optional col is missing)
all_needed <- c(cont_vars, other_vars, target)
missing_cols <- setdiff(all_needed, names(df_fe))
if (length(missing_cols) > 0) {
  warning("These columns are missing and will be skipped: ",
          paste(missing_cols, collapse = ", "))
  all_needed <- setdiff(all_needed, missing_cols)
  cont_vars  <- setdiff(cont_vars,  missing_cols)
  other_vars <- setdiff(other_vars, missing_cols)
}

model_data <- df_fe |>
  dplyr::select(dplyr::all_of(all_needed)) |>
  tidyr::drop_na()

# scale continuous variables
model_data <- model_data |>
  dplyr::mutate(dplyr::across(dplyr::all_of(cont_vars), ~ as.numeric(scale(.x))))

# preview dataset
dim(model_data)
summary(model_data[[target]])
model_data |> dplyr::slice_head(n = 5)


```

```{r}
# Step 5 — Baseline linear regression model (OLS)

library(broom)
library(car)

# build OLS model

model_ols <- lm(log_sale_price ~ ., data = model_data)

# model summary

summary(model_ols)

# Variance Inflation Factor (VIF) check for multicollinearity

vif_vals <- car::vif(model_ols)
sort(vif_vals, decreasing = TRUE)[1:10]

# extract tidy table of coefficients

tidy(model_ols) |>
arrange(p.value) |>
head(10)

```

```{r}
# Step 5b — Residual diagnostics

par(mfrow = c(2, 2))
plot(model_ols)

# Shapiro–Wilk test for normality of residuals

# Residuals

resid_ols <- residuals(model_ols)

# Shapiro–Wilk on a random subsample of size 5000

set.seed(5080)
resid_sub <- sample(resid_ols, size = 5000, replace = FALSE)
shapiro.test(resid_sub)



# Breusch–Pagan test for heteroskedasticity

library(lmtest)
bptest(model_ols)

```

```{r}
# Step 6 — Regularization (Lasso / Ridge)

library(glmnet)
library(caret)

library(dplyr)
# prepare matrices

X <- model_data |> select(-log_sale_price) |> as.matrix()
y <- model_data$log_sale_price

set.seed(5080)
cv_lasso <- cv.glmnet(X, y, alpha = 1, nfolds = 10)
cv_ridge <- cv.glmnet(X, y, alpha = 0, nfolds = 10)

# plot CV errors

par(mfrow = c(1, 2))
plot(cv_lasso, main = "Lasso 10-fold CV")
plot(cv_ridge, main = "Ridge 10-fold CV")
par(mfrow = c(1, 1))

# best lambda and test error

tibble(
Model = c("Lasso", "Ridge"),
Lambda_min = c(cv_lasso$lambda.min, cv_ridge$lambda.min),
Lambda_1se = c(cv_lasso$lambda.1se, cv_ridge$lambda.1se),
CV_MSE = c(min(cv_lasso$cvm), min(cv_ridge$cvm))
)


```

```{r}
# Step 6b — Extract coefficients and variable importance

coef_lasso <- coef(cv_lasso, s = "lambda.min") |> as.matrix()
coef_ridge <- coef(cv_ridge, s = "lambda.min") |> as.matrix()

# top absolute coefficients (Lasso)

lasso_imp <- tibble(
feature = rownames(coef_lasso),
coef = as.numeric(coef_lasso)
) |>
filter(feature != "(Intercept)") |>
arrange(desc(abs(coef))) |>
slice_head(n = 10)

lasso_imp


```

```{r}
# Step 7 — Train/test split and metrics helpers

set.seed(5080)

n <- nrow(model_data)
idx_train <- sample.int(n, size = floor(0.8 * n))
train <- model_data[idx_train, ]
test  <- model_data[-idx_train, ]

rmse <- function(actual, pred) sqrt(mean((actual - pred)^2))
mae  <- function(actual, pred) mean(abs(actual - pred))
r2   <- function(actual, pred) 1 - sum((actual - pred)^2) / sum((actual - mean(actual))^2)

```

```{r}
# Step 7a — Random Forest (ranger)

if (!requireNamespace("ranger", quietly = TRUE)) install.packages("ranger")
library(ranger)

# build RF with OOB error; tune a few core hyperparameters

rf_fit <- ranger(
formula         = log_sale_price ~ .,
data            = train,
num.trees       = 1000,
mtry            = floor(sqrt(ncol(train) - 1)),
min.node.size   = 5,
respect.unordered.factors = "order",
importance      = "impurity",
seed            = 5080
)

# OOB RMSE (training proxy)

rf_oob_rmse <- sqrt(rf_fit$prediction.error)

# Test predictions and metrics

rf_pred <- predict(rf_fit, data = test)$predictions
rf_rmse <- rmse(test$log_sale_price, rf_pred)
rf_mae  <- mae(test$log_sale_price, rf_pred)
rf_r2   <- r2(test$log_sale_price, rf_pred)

data.frame(
Model = "Random Forest",
OOB_RMSE = rf_oob_rmse,
Test_RMSE = rf_rmse,
Test_MAE = rf_mae,
Test_R2 = rf_r2
)

```

```{r}
# Step 7a.1 — RF variable importance (top 15)

rf_imp <- as.data.frame(rf_fit$variable.importance)
rf_imp$feature <- rownames(rf_imp)
colnames(rf_imp)[1] <- "importance"
rf_imp <- rf_imp[order(rf_imp$importance, decreasing = TRUE), ]
head(rf_imp, 15)

```

```{r}
# Step 7b — XGBoost (with CV to pick nrounds)

if (!requireNamespace("xgboost", quietly = TRUE)) install.packages("xgboost")
if (!requireNamespace("Matrix", quietly = TRUE))  install.packages("Matrix")

library(xgboost)
library(Matrix)

# build sparse design matrices (drop target)

x_train <- as.matrix(train[, !(names(train) %in% "log_sale_price")])
x_test  <- as.matrix(test[,  !(names(test)  %in% "log_sale_price")])
y_train <- train$log_sale_price
y_test  <- test$log_sale_price

dtrain <- xgb.DMatrix(data = x_train, label = y_train)
dtest  <- xgb.DMatrix(data = x_test,  label = y_test)

params <- list(
objective = "reg:squarederror",
eval_metric = "rmse",
eta = 0.05,
max_depth = 6,
subsample = 0.8,
colsample_bytree = 0.8,
min_child_weight = 5
)

set.seed(5080)
cv_res <- xgb.cv(
params = params,
data = dtrain,
nrounds = 3000,
nfold = 5,
early_stopping_rounds = 50,
verbose = 0
)

best_nrounds <- cv_res$best_iteration

xgb_fit <- xgb.train(
params = params,
data = dtrain,
nrounds = best_nrounds,
watchlist = list(train = dtrain),
verbose = 0
)

xgb_pred <- predict(xgb_fit, dtest)

xgb_rmse <- rmse(y_test, xgb_pred)
xgb_mae  <- mae(y_test, xgb_pred)
xgb_r2   <- r2(y_test, xgb_pred)

data.frame(
Model = "XGBoost",
Best_nrounds = best_nrounds,
Test_RMSE = xgb_rmse,
Test_MAE = xgb_mae,
Test_R2 = xgb_r2
)

```

```{r}
xgb_imp <- xgb.importance(model = xgb_fit)
head(xgb_imp, 15)
```

```{r}
# Phase 5 — 10-fold Cross-Validation across 4 models

library(caret)
library(dplyr)
library(ggplot2)

# Ensure factors for fixed effects

model_data_cv <- model_data %>%
mutate(
sale_year = factor(sale_year),
sale_quarter = factor(sale_quarter)
)

# Define progressive feature sets

vars_structural <- c(
"number_of_bedrooms","number_of_bathrooms","total_livable_area",
"house_age","exterior_condition","interior_condition"
)

vars_census <- c(
"median_incomeE","per_cap_incomeE","PCTPOVERTY","PCBACHMORE"
)

vars_spatial <- c(
"dist_to_park_ft_log","dist_transit_ft_log","dist_to_hospital_ft_log",
"dist_to_nearest_school_ft_log","crime_count_15min_walk",
"park_within_15min_walk","transit_15min_walk","hospitals_15min_walk","schools_within_15min_walk"
)

# Build model formulas

form_structural <- as.formula(
paste("log_sale_price ~", paste(vars_structural, collapse = " + "))
)

form_struct_census <- as.formula(
paste("log_sale_price ~", paste(c(vars_structural, vars_census), collapse = " + "))
)

form_struct_census_spatial <- as.formula(
paste("log_sale_price ~", paste(c(vars_structural, vars_census, vars_spatial), collapse = " + "))
)

# Interactions & fixed effects (examples: bedrooms*bathrooms; area with crime; year/quarter FE)

form_inter_fe <- as.formula(
paste(
"log_sale_price ~",
paste(c(vars_structural, vars_census, vars_spatial), collapse = " + "),
"+ number_of_bedrooms:number_of_bathrooms",
"+ total_livable_area:crime_count_15min_walk",
"+ sale_year + sale_quarter"
)
)

# Custom summary to add MAE

maeSummary <- function(data, lev = NULL, model = NULL) {
out <- c(
RMSE = sqrt(mean((data$obs - data$pred)^2)),
Rsquared = cor(data$obs, data$pred)^2,
MAE = mean(abs(data$obs - data$pred))
)
out
}

set.seed(5080)
ctrl <- trainControl(
method = "cv",
number = 10,
savePredictions = "final",
summaryFunction = maeSummary
)

# Train four models with 10-fold CV (linear baseline for fair comparison)

set.seed(5080)
fit_structural <- train(
form_structural, data = model_data_cv,
method = "lm", trControl = ctrl, metric = "RMSE"
)

set.seed(5080)
fit_struct_census <- train(
form_struct_census, data = model_data_cv,
method = "lm", trControl = ctrl, metric = "RMSE"
)

set.seed(5080)
fit_struct_census_spatial <- train(
form_struct_census_spatial, data = model_data_cv,
method = "lm", trControl = ctrl, metric = "RMSE"
)

set.seed(5080)
fit_inter_fe <- train(
form_inter_fe, data = model_data_cv,
method = "lm", trControl = ctrl, metric = "RMSE"
)

# Collect CV results (RMSE, R^2, MAE)

cv_table <- bind_rows(
tibble(Model = "Structural Only",
RMSE = fit_structural$results$RMSE[1],
Rsq  = fit_structural$results$Rsquared[1],
MAE  = fit_structural$results$MAE[1]),
tibble(Model = "+ Census",
RMSE = fit_struct_census$results$RMSE[1],
Rsq  = fit_struct_census$results$Rsquared[1],
MAE  = fit_struct_census$results$MAE[1]),
tibble(Model = "+ Spatial",
RMSE = fit_struct_census_spatial$results$RMSE[1],
Rsq  = fit_struct_census_spatial$results$Rsquared[1],
MAE  = fit_struct_census_spatial$results$MAE[1]),
tibble(Model = "+ Interactions / FE",
RMSE = fit_inter_fe$results$RMSE[1],
Rsq  = fit_inter_fe$results$Rsquared[1],
MAE  = fit_inter_fe$results$MAE[1])
) %>%
arrange(RMSE)

cv_table

```

```{r}
# Predicted vs Actual (OOF) for the best model by RMSE (usually Interactions/FE)

best_fit <- fit_inter_fe
oof <- best_fit$pred %>%
transmute(pred = pred, obs = obs)

ggplot(oof, aes(x = obs, y = pred)) +
geom_point(alpha = 0.15) +
geom_abline(slope = 1, intercept = 0, linetype = 2) +
labs(
title = "Predicted vs. Actual (Out-of-Fold) — Interactions/FE Model",
x = "Actual log(Sale Price)", y = "Predicted log(Sale Price)"
) +
theme_minimal()

```

```{r}
# Optional: nicely formatted comparison table for slides

cv_table %>%
mutate(
RMSE = round(RMSE, 3),
Rsq  = round(Rsq, 3),
MAE  = round(MAE, 3)
)

```

```{r}
# Phase 6 — Model Diagnostics for the best linear spec (Interactions / FE)

# Refit the Interactions/FE model on the full dataset to get residuals cleanly

model_inter_fe <- lm(form_inter_fe, data = model_data_cv)

summary(model_inter_fe)  # optional: quick overview

```

```{r}
# 6.1 Residual vs Fitted (linearity & homoscedasticity check)

# 6.1 Residual vs Fitted (linearity & homoscedasticity check)
par(mfrow = c(1, 1))
plot(
  model_inter_fe$fitted.values,
  resid(model_inter_fe),
  pch = 16, col = rgb(0, 0, 0, 0.25),
  xlab = "Fitted Values",
  ylab = "Residuals",
  main = "Residuals vs Fitted"
)
abline(h = 0, lty = 2, col = 2)


```

```{r}
# 6.2 Q-Q plot (normality of residuals)

qqnorm(resid(model_inter_fe), main = "Q-Q Plot of Residuals")
qqline(resid(model_inter_fe), col = 2, lwd = 2)

```

```{r}
# 6.3 Scale-Location (residual spread vs fitted — variance stabilization)

plot(model_inter_fe, which = 3)  # Scale-Location built-in plot

```

```{r}
# 6.4 Cook's distance (influential observations)

n <- nrow(model_data_cv)
cook <- cooks.distance(model_inter_fe)
thr <- 4 / n  # common rule of thumb

# Basic summary

cat("Cook's D threshold (4/n):", round(thr, 6), "\n")
cat("Number of points above threshold:", sum(cook > thr), "\n")

# Plot Cook's D with threshold

plot(cook, type = "h", col = "gray40",
main = "Cook's Distance", ylab = "Cook's D")
abline(h = thr, col = 2, lty = 2)

# Optionally list top-10 influential indices

head(order(cook, decreasing = TRUE), 10)

```

```{r}
# 6.5 (Optional) Heteroskedasticity test (Breusch–Pagan)

if (!requireNamespace("lmtest", quietly = TRUE)) install.packages("lmtest")
library(lmtest)
bptest(model_inter_fe)

```

```{r}
# 6.6 (Optional) Multicollinearity check (VIF)

if (!requireNamespace("car", quietly = TRUE)) install.packages("car")
library(car)
vif_vals_diag <- car::vif(model_inter_fe)
sort(vif_vals_diag, decreasing = TRUE)[1:10]

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```
